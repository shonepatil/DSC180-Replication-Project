{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.features.build_features import load_data\n",
    "from src.model.train import train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Twitch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading musae_ENGB_ dataset...\n",
      "Graph Info:\n",
      " Name: G\n",
      "Type: Graph\n",
      "Number of nodes: 7126\n",
      "Number of edges: 35324\n",
      "Average degree:   9.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s3patil/DSC 180/DSC180-Replication-Project/src/model/utils.py:13: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2897.)\n",
      "  evals, evecs = torch.eig (m, eigenvectors = True)  # get eigendecomposition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  torch.Size([7126, 7126])\n",
      "\n",
      "Shape of X:  torch.Size([7126, 3])\n",
      "\n",
      "Adjacency Matrix (A):\n",
      " tensor([[0.1250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0500, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2500, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0476, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3333]])\n",
      "\n",
      "Node Features Matrix (X):\n",
      " tensor([[-0.0937, -0.0620, -0.2387],\n",
      "        [ 0.1469, -0.0640, -0.2387],\n",
      "        [-1.5768, -0.0495, -0.2387],\n",
      "        ...,\n",
      "        [-0.1517, -0.0639, -0.2387],\n",
      "        [ 0.7370, -0.0631, -0.2387],\n",
      "        [ 0.3846, -0.0640, -0.2387]])\n"
     ]
    }
   ],
   "source": [
    "A, X, y, idx_train, idx_val, idx_test = load_data(\"../data/twitch/\", \"musae_ENGB_\", 1000, 2000, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.7165 acc_train: 0.5010 loss_val: 0.7096 acc_val: 0.4780 time: 0.0056s\n",
      "Epoch: 0002 loss_train: 0.7310 acc_train: 0.5050 loss_val: 0.7087 acc_val: 0.4840 time: 0.0054s\n",
      "Epoch: 0003 loss_train: 0.7169 acc_train: 0.4850 loss_val: 0.7079 acc_val: 0.4870 time: 0.0045s\n",
      "Epoch: 0004 loss_train: 0.7194 acc_train: 0.5030 loss_val: 0.7071 acc_val: 0.4925 time: 0.0044s\n",
      "Epoch: 0005 loss_train: 0.7090 acc_train: 0.5250 loss_val: 0.7063 acc_val: 0.4950 time: 0.0041s\n",
      "Epoch: 0006 loss_train: 0.7109 acc_train: 0.5050 loss_val: 0.7055 acc_val: 0.4990 time: 0.0042s\n",
      "Epoch: 0007 loss_train: 0.7167 acc_train: 0.4920 loss_val: 0.7048 acc_val: 0.5005 time: 0.0040s\n",
      "Epoch: 0008 loss_train: 0.7131 acc_train: 0.5200 loss_val: 0.7042 acc_val: 0.5055 time: 0.0039s\n",
      "Epoch: 0009 loss_train: 0.7161 acc_train: 0.4970 loss_val: 0.7035 acc_val: 0.5035 time: 0.0038s\n",
      "Epoch: 0010 loss_train: 0.7174 acc_train: 0.4990 loss_val: 0.7030 acc_val: 0.5040 time: 0.0036s\n",
      "Epoch: 0011 loss_train: 0.7155 acc_train: 0.4700 loss_val: 0.7024 acc_val: 0.5080 time: 0.0036s\n",
      "Epoch: 0012 loss_train: 0.7109 acc_train: 0.5000 loss_val: 0.7019 acc_val: 0.5090 time: 0.0038s\n",
      "Epoch: 0013 loss_train: 0.7156 acc_train: 0.4790 loss_val: 0.7014 acc_val: 0.5100 time: 0.0034s\n",
      "Epoch: 0014 loss_train: 0.7169 acc_train: 0.4890 loss_val: 0.7010 acc_val: 0.5100 time: 0.0037s\n",
      "Epoch: 0015 loss_train: 0.7106 acc_train: 0.5080 loss_val: 0.7006 acc_val: 0.5075 time: 0.0035s\n",
      "Epoch: 0016 loss_train: 0.7204 acc_train: 0.5010 loss_val: 0.7002 acc_val: 0.5105 time: 0.0036s\n",
      "Epoch: 0017 loss_train: 0.6984 acc_train: 0.5350 loss_val: 0.6998 acc_val: 0.5080 time: 0.0037s\n",
      "Epoch: 0018 loss_train: 0.7042 acc_train: 0.5110 loss_val: 0.6994 acc_val: 0.5070 time: 0.0034s\n",
      "Epoch: 0019 loss_train: 0.6992 acc_train: 0.5220 loss_val: 0.6991 acc_val: 0.5070 time: 0.0035s\n",
      "Epoch: 0020 loss_train: 0.7094 acc_train: 0.4920 loss_val: 0.6988 acc_val: 0.5055 time: 0.0032s\n",
      "Epoch: 0021 loss_train: 0.7086 acc_train: 0.5250 loss_val: 0.6985 acc_val: 0.5025 time: 0.0033s\n",
      "Epoch: 0022 loss_train: 0.7025 acc_train: 0.5270 loss_val: 0.6982 acc_val: 0.5010 time: 0.0032s\n",
      "Epoch: 0023 loss_train: 0.7069 acc_train: 0.5050 loss_val: 0.6979 acc_val: 0.5030 time: 0.0031s\n",
      "Epoch: 0024 loss_train: 0.7125 acc_train: 0.5020 loss_val: 0.6977 acc_val: 0.5050 time: 0.0032s\n",
      "Epoch: 0025 loss_train: 0.6966 acc_train: 0.5160 loss_val: 0.6974 acc_val: 0.5075 time: 0.0029s\n",
      "Epoch: 0026 loss_train: 0.7039 acc_train: 0.5260 loss_val: 0.6972 acc_val: 0.5090 time: 0.0029s\n",
      "Epoch: 0027 loss_train: 0.7125 acc_train: 0.5060 loss_val: 0.6970 acc_val: 0.5075 time: 0.0029s\n",
      "Epoch: 0028 loss_train: 0.7086 acc_train: 0.5220 loss_val: 0.6968 acc_val: 0.5100 time: 0.0032s\n",
      "Epoch: 0029 loss_train: 0.7145 acc_train: 0.5090 loss_val: 0.6966 acc_val: 0.5105 time: 0.0030s\n",
      "Epoch: 0030 loss_train: 0.6973 acc_train: 0.5340 loss_val: 0.6964 acc_val: 0.5110 time: 0.0030s\n",
      "Epoch: 0031 loss_train: 0.7032 acc_train: 0.5140 loss_val: 0.6962 acc_val: 0.5130 time: 0.0028s\n",
      "Epoch: 0032 loss_train: 0.7085 acc_train: 0.5060 loss_val: 0.6960 acc_val: 0.5100 time: 0.0031s\n",
      "Epoch: 0033 loss_train: 0.7073 acc_train: 0.5260 loss_val: 0.6958 acc_val: 0.5100 time: 0.0028s\n",
      "Epoch: 0034 loss_train: 0.7029 acc_train: 0.5330 loss_val: 0.6956 acc_val: 0.5115 time: 0.0029s\n",
      "Epoch: 0035 loss_train: 0.7073 acc_train: 0.5100 loss_val: 0.6955 acc_val: 0.5150 time: 0.0031s\n",
      "Epoch: 0036 loss_train: 0.7059 acc_train: 0.5250 loss_val: 0.6953 acc_val: 0.5165 time: 0.0030s\n",
      "Epoch: 0037 loss_train: 0.6984 acc_train: 0.5260 loss_val: 0.6951 acc_val: 0.5175 time: 0.0028s\n",
      "Epoch: 0038 loss_train: 0.6957 acc_train: 0.5160 loss_val: 0.6950 acc_val: 0.5155 time: 0.0032s\n",
      "Epoch: 0039 loss_train: 0.7085 acc_train: 0.5040 loss_val: 0.6948 acc_val: 0.5180 time: 0.0029s\n",
      "Epoch: 0040 loss_train: 0.7062 acc_train: 0.5260 loss_val: 0.6947 acc_val: 0.5180 time: 0.0029s\n",
      "Epoch: 0041 loss_train: 0.7054 acc_train: 0.4970 loss_val: 0.6946 acc_val: 0.5180 time: 0.0034s\n",
      "Epoch: 0042 loss_train: 0.7071 acc_train: 0.4990 loss_val: 0.6944 acc_val: 0.5195 time: 0.0030s\n",
      "Epoch: 0043 loss_train: 0.7038 acc_train: 0.5050 loss_val: 0.6943 acc_val: 0.5225 time: 0.0030s\n",
      "Epoch: 0044 loss_train: 0.7065 acc_train: 0.5310 loss_val: 0.6942 acc_val: 0.5215 time: 0.0030s\n",
      "Epoch: 0045 loss_train: 0.6999 acc_train: 0.5220 loss_val: 0.6941 acc_val: 0.5215 time: 0.0029s\n",
      "Epoch: 0046 loss_train: 0.7001 acc_train: 0.5530 loss_val: 0.6939 acc_val: 0.5195 time: 0.0029s\n",
      "Epoch: 0047 loss_train: 0.7042 acc_train: 0.5260 loss_val: 0.6938 acc_val: 0.5255 time: 0.0030s\n",
      "Epoch: 0048 loss_train: 0.7032 acc_train: 0.5180 loss_val: 0.6937 acc_val: 0.5255 time: 0.0027s\n",
      "Epoch: 0049 loss_train: 0.7066 acc_train: 0.5240 loss_val: 0.6936 acc_val: 0.5240 time: 0.0031s\n",
      "Epoch: 0050 loss_train: 0.7014 acc_train: 0.5280 loss_val: 0.6935 acc_val: 0.5270 time: 0.0031s\n",
      "Epoch: 0051 loss_train: 0.7016 acc_train: 0.5100 loss_val: 0.6934 acc_val: 0.5320 time: 0.0028s\n",
      "Epoch: 0052 loss_train: 0.7072 acc_train: 0.5100 loss_val: 0.6933 acc_val: 0.5325 time: 0.0029s\n",
      "Epoch: 0053 loss_train: 0.6992 acc_train: 0.5290 loss_val: 0.6932 acc_val: 0.5350 time: 0.0030s\n",
      "Epoch: 0054 loss_train: 0.6962 acc_train: 0.5530 loss_val: 0.6931 acc_val: 0.5345 time: 0.0029s\n",
      "Epoch: 0055 loss_train: 0.7102 acc_train: 0.5140 loss_val: 0.6930 acc_val: 0.5360 time: 0.0030s\n",
      "Epoch: 0056 loss_train: 0.6990 acc_train: 0.5400 loss_val: 0.6929 acc_val: 0.5355 time: 0.0029s\n",
      "Epoch: 0057 loss_train: 0.6959 acc_train: 0.5390 loss_val: 0.6928 acc_val: 0.5350 time: 0.0029s\n",
      "Epoch: 0058 loss_train: 0.7052 acc_train: 0.5220 loss_val: 0.6927 acc_val: 0.5335 time: 0.0029s\n",
      "Epoch: 0059 loss_train: 0.7015 acc_train: 0.5270 loss_val: 0.6926 acc_val: 0.5360 time: 0.0030s\n",
      "Epoch: 0060 loss_train: 0.7000 acc_train: 0.5300 loss_val: 0.6925 acc_val: 0.5380 time: 0.0030s\n",
      "Epoch: 0061 loss_train: 0.7017 acc_train: 0.5300 loss_val: 0.6924 acc_val: 0.5385 time: 0.0029s\n",
      "Epoch: 0062 loss_train: 0.6979 acc_train: 0.5420 loss_val: 0.6924 acc_val: 0.5410 time: 0.0029s\n",
      "Epoch: 0063 loss_train: 0.6961 acc_train: 0.5310 loss_val: 0.6923 acc_val: 0.5425 time: 0.0029s\n",
      "Epoch: 0064 loss_train: 0.7023 acc_train: 0.5190 loss_val: 0.6922 acc_val: 0.5430 time: 0.0030s\n",
      "Epoch: 0065 loss_train: 0.7015 acc_train: 0.5360 loss_val: 0.6922 acc_val: 0.5435 time: 0.0030s\n",
      "Epoch: 0066 loss_train: 0.6960 acc_train: 0.5160 loss_val: 0.6921 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0067 loss_train: 0.6971 acc_train: 0.5380 loss_val: 0.6920 acc_val: 0.5425 time: 0.0029s\n",
      "Epoch: 0068 loss_train: 0.6917 acc_train: 0.5520 loss_val: 0.6920 acc_val: 0.5410 time: 0.0028s\n",
      "Epoch: 0069 loss_train: 0.6936 acc_train: 0.5400 loss_val: 0.6919 acc_val: 0.5415 time: 0.0027s\n",
      "Epoch: 0070 loss_train: 0.7043 acc_train: 0.5250 loss_val: 0.6919 acc_val: 0.5425 time: 0.0031s\n",
      "Epoch: 0071 loss_train: 0.6897 acc_train: 0.5480 loss_val: 0.6918 acc_val: 0.5420 time: 0.0028s\n",
      "Epoch: 0072 loss_train: 0.7002 acc_train: 0.5460 loss_val: 0.6918 acc_val: 0.5420 time: 0.0031s\n",
      "Epoch: 0073 loss_train: 0.6966 acc_train: 0.5470 loss_val: 0.6917 acc_val: 0.5420 time: 0.0028s\n",
      "Epoch: 0074 loss_train: 0.7034 acc_train: 0.5270 loss_val: 0.6917 acc_val: 0.5415 time: 0.0030s\n",
      "Epoch: 0075 loss_train: 0.7030 acc_train: 0.5320 loss_val: 0.6916 acc_val: 0.5415 time: 0.0028s\n",
      "Epoch: 0076 loss_train: 0.6886 acc_train: 0.5480 loss_val: 0.6916 acc_val: 0.5420 time: 0.0030s\n",
      "Epoch: 0077 loss_train: 0.7018 acc_train: 0.5270 loss_val: 0.6915 acc_val: 0.5430 time: 0.0029s\n",
      "Epoch: 0078 loss_train: 0.7048 acc_train: 0.5360 loss_val: 0.6915 acc_val: 0.5440 time: 0.0031s\n",
      "Epoch: 0079 loss_train: 0.6927 acc_train: 0.5380 loss_val: 0.6914 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0080 loss_train: 0.7043 acc_train: 0.5170 loss_val: 0.6914 acc_val: 0.5435 time: 0.0031s\n",
      "Epoch: 0081 loss_train: 0.7033 acc_train: 0.5200 loss_val: 0.6914 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0082 loss_train: 0.6983 acc_train: 0.5430 loss_val: 0.6913 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0083 loss_train: 0.6964 acc_train: 0.5410 loss_val: 0.6913 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0084 loss_train: 0.6978 acc_train: 0.5220 loss_val: 0.6913 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0085 loss_train: 0.7050 acc_train: 0.5350 loss_val: 0.6912 acc_val: 0.5440 time: 0.0027s\n",
      "Epoch: 0086 loss_train: 0.6982 acc_train: 0.5350 loss_val: 0.6912 acc_val: 0.5440 time: 0.0030s\n",
      "Epoch: 0087 loss_train: 0.7018 acc_train: 0.5270 loss_val: 0.6912 acc_val: 0.5440 time: 0.0027s\n",
      "Epoch: 0088 loss_train: 0.6934 acc_train: 0.5380 loss_val: 0.6911 acc_val: 0.5440 time: 0.0028s\n",
      "Epoch: 0089 loss_train: 0.7002 acc_train: 0.5360 loss_val: 0.6911 acc_val: 0.5440 time: 0.0026s\n",
      "Epoch: 0090 loss_train: 0.7028 acc_train: 0.5050 loss_val: 0.6911 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0091 loss_train: 0.7003 acc_train: 0.5500 loss_val: 0.6910 acc_val: 0.5440 time: 0.0032s\n",
      "Epoch: 0092 loss_train: 0.6890 acc_train: 0.5520 loss_val: 0.6910 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0093 loss_train: 0.6903 acc_train: 0.5260 loss_val: 0.6910 acc_val: 0.5440 time: 0.0026s\n",
      "Epoch: 0094 loss_train: 0.7020 acc_train: 0.5210 loss_val: 0.6910 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0095 loss_train: 0.6959 acc_train: 0.5360 loss_val: 0.6909 acc_val: 0.5440 time: 0.0027s\n",
      "Epoch: 0096 loss_train: 0.6935 acc_train: 0.5290 loss_val: 0.6909 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0097 loss_train: 0.6948 acc_train: 0.5380 loss_val: 0.6909 acc_val: 0.5445 time: 0.0026s\n",
      "Epoch: 0098 loss_train: 0.6994 acc_train: 0.5270 loss_val: 0.6909 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0099 loss_train: 0.7002 acc_train: 0.5510 loss_val: 0.6909 acc_val: 0.5445 time: 0.0027s\n",
      "Epoch: 0100 loss_train: 0.6928 acc_train: 0.5420 loss_val: 0.6908 acc_val: 0.5445 time: 0.0026s\n",
      "Epoch: 0101 loss_train: 0.7046 acc_train: 0.5350 loss_val: 0.6908 acc_val: 0.5440 time: 0.0028s\n",
      "Epoch: 0102 loss_train: 0.6973 acc_train: 0.5160 loss_val: 0.6908 acc_val: 0.5440 time: 0.0028s\n",
      "Epoch: 0103 loss_train: 0.6875 acc_train: 0.5350 loss_val: 0.6908 acc_val: 0.5440 time: 0.0030s\n",
      "Epoch: 0104 loss_train: 0.7028 acc_train: 0.5180 loss_val: 0.6908 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0105 loss_train: 0.6946 acc_train: 0.5470 loss_val: 0.6908 acc_val: 0.5440 time: 0.0031s\n",
      "Epoch: 0106 loss_train: 0.7056 acc_train: 0.5280 loss_val: 0.6908 acc_val: 0.5440 time: 0.0027s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0107 loss_train: 0.6934 acc_train: 0.5480 loss_val: 0.6908 acc_val: 0.5440 time: 0.0033s\n",
      "Epoch: 0108 loss_train: 0.6989 acc_train: 0.5230 loss_val: 0.6908 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0109 loss_train: 0.7070 acc_train: 0.5300 loss_val: 0.6908 acc_val: 0.5445 time: 0.0027s\n",
      "Epoch: 0110 loss_train: 0.6952 acc_train: 0.5480 loss_val: 0.6908 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0111 loss_train: 0.6915 acc_train: 0.5570 loss_val: 0.6908 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0112 loss_train: 0.6994 acc_train: 0.5220 loss_val: 0.6908 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0113 loss_train: 0.6963 acc_train: 0.5390 loss_val: 0.6908 acc_val: 0.5445 time: 0.0028s\n",
      "Epoch: 0114 loss_train: 0.7027 acc_train: 0.5260 loss_val: 0.6908 acc_val: 0.5445 time: 0.0030s\n",
      "Epoch: 0115 loss_train: 0.6965 acc_train: 0.5300 loss_val: 0.6908 acc_val: 0.5445 time: 0.0030s\n",
      "Epoch: 0116 loss_train: 0.6983 acc_train: 0.5230 loss_val: 0.6907 acc_val: 0.5445 time: 0.0033s\n",
      "Epoch: 0117 loss_train: 0.6974 acc_train: 0.5470 loss_val: 0.6907 acc_val: 0.5445 time: 0.0041s\n",
      "Epoch: 0118 loss_train: 0.7010 acc_train: 0.5350 loss_val: 0.6907 acc_val: 0.5445 time: 0.0038s\n",
      "Epoch: 0119 loss_train: 0.7072 acc_train: 0.5160 loss_val: 0.6907 acc_val: 0.5445 time: 0.0031s\n",
      "Epoch: 0120 loss_train: 0.6877 acc_train: 0.5690 loss_val: 0.6907 acc_val: 0.5445 time: 0.0038s\n",
      "Epoch: 0121 loss_train: 0.6938 acc_train: 0.5240 loss_val: 0.6907 acc_val: 0.5445 time: 0.0037s\n",
      "Epoch: 0122 loss_train: 0.6952 acc_train: 0.5380 loss_val: 0.6907 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0123 loss_train: 0.6974 acc_train: 0.5220 loss_val: 0.6907 acc_val: 0.5440 time: 0.0030s\n",
      "Epoch: 0124 loss_train: 0.6966 acc_train: 0.5330 loss_val: 0.6907 acc_val: 0.5440 time: 0.0037s\n",
      "Epoch: 0125 loss_train: 0.6966 acc_train: 0.5250 loss_val: 0.6907 acc_val: 0.5440 time: 0.0033s\n",
      "Epoch: 0126 loss_train: 0.6942 acc_train: 0.5430 loss_val: 0.6907 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0127 loss_train: 0.6944 acc_train: 0.5360 loss_val: 0.6907 acc_val: 0.5440 time: 0.0027s\n",
      "Epoch: 0128 loss_train: 0.6901 acc_train: 0.5490 loss_val: 0.6908 acc_val: 0.5440 time: 0.0027s\n",
      "Epoch: 0129 loss_train: 0.6912 acc_train: 0.5530 loss_val: 0.6908 acc_val: 0.5440 time: 0.0029s\n",
      "Epoch: 0130 loss_train: 0.6992 acc_train: 0.5360 loss_val: 0.6908 acc_val: 0.5445 time: 0.0027s\n",
      "Epoch: 0131 loss_train: 0.6912 acc_train: 0.5520 loss_val: 0.6908 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0132 loss_train: 0.6966 acc_train: 0.5470 loss_val: 0.6908 acc_val: 0.5445 time: 0.0036s\n",
      "Epoch: 0133 loss_train: 0.6942 acc_train: 0.5250 loss_val: 0.6908 acc_val: 0.5455 time: 0.0031s\n",
      "Epoch: 0134 loss_train: 0.6945 acc_train: 0.5410 loss_val: 0.6908 acc_val: 0.5450 time: 0.0036s\n",
      "Epoch: 0135 loss_train: 0.7020 acc_train: 0.5300 loss_val: 0.6908 acc_val: 0.5445 time: 0.0032s\n",
      "Epoch: 0136 loss_train: 0.6998 acc_train: 0.5190 loss_val: 0.6908 acc_val: 0.5445 time: 0.0029s\n",
      "Epoch: 0137 loss_train: 0.6994 acc_train: 0.5340 loss_val: 0.6908 acc_val: 0.5445 time: 0.0036s\n",
      "Epoch: 0138 loss_train: 0.6932 acc_train: 0.5380 loss_val: 0.6908 acc_val: 0.5445 time: 0.0036s\n",
      "Epoch: 0139 loss_train: 0.6966 acc_train: 0.5460 loss_val: 0.6908 acc_val: 0.5445 time: 0.0030s\n",
      "Epoch: 0140 loss_train: 0.6845 acc_train: 0.5570 loss_val: 0.6908 acc_val: 0.5445 time: 0.0035s\n",
      "Epoch: 0141 loss_train: 0.6907 acc_train: 0.5430 loss_val: 0.6909 acc_val: 0.5445 time: 0.0037s\n",
      "Epoch: 0142 loss_train: 0.7016 acc_train: 0.5290 loss_val: 0.6909 acc_val: 0.5445 time: 0.0036s\n",
      "Epoch: 0143 loss_train: 0.6956 acc_train: 0.5400 loss_val: 0.6909 acc_val: 0.5445 time: 0.0035s\n",
      "Epoch: 0144 loss_train: 0.6975 acc_train: 0.5390 loss_val: 0.6909 acc_val: 0.5450 time: 0.0037s\n",
      "Epoch: 0145 loss_train: 0.6899 acc_train: 0.5520 loss_val: 0.6909 acc_val: 0.5450 time: 0.0036s\n",
      "Epoch: 0146 loss_train: 0.6944 acc_train: 0.5350 loss_val: 0.6909 acc_val: 0.5450 time: 0.0033s\n",
      "Epoch: 0147 loss_train: 0.6926 acc_train: 0.5440 loss_val: 0.6909 acc_val: 0.5440 time: 0.0031s\n",
      "Epoch: 0148 loss_train: 0.6972 acc_train: 0.5320 loss_val: 0.6909 acc_val: 0.5435 time: 0.0030s\n",
      "Epoch: 0149 loss_train: 0.6901 acc_train: 0.5480 loss_val: 0.6909 acc_val: 0.5440 time: 0.0035s\n",
      "Epoch: 0150 loss_train: 0.6878 acc_train: 0.5550 loss_val: 0.6909 acc_val: 0.5445 time: 0.0037s\n",
      "Epoch: 0151 loss_train: 0.6964 acc_train: 0.5430 loss_val: 0.6909 acc_val: 0.5440 time: 0.0034s\n",
      "Epoch: 0152 loss_train: 0.7010 acc_train: 0.5170 loss_val: 0.6909 acc_val: 0.5440 time: 0.0030s\n",
      "Epoch: 0153 loss_train: 0.6917 acc_train: 0.5380 loss_val: 0.6909 acc_val: 0.5435 time: 0.0031s\n",
      "Epoch: 0154 loss_train: 0.6971 acc_train: 0.5380 loss_val: 0.6909 acc_val: 0.5435 time: 0.0032s\n",
      "Epoch: 0155 loss_train: 0.6899 acc_train: 0.5340 loss_val: 0.6909 acc_val: 0.5435 time: 0.0033s\n",
      "Epoch: 0156 loss_train: 0.6932 acc_train: 0.5400 loss_val: 0.6909 acc_val: 0.5435 time: 0.0034s\n",
      "Epoch: 0157 loss_train: 0.6871 acc_train: 0.5470 loss_val: 0.6909 acc_val: 0.5435 time: 0.0032s\n",
      "Epoch: 0158 loss_train: 0.6977 acc_train: 0.5400 loss_val: 0.6909 acc_val: 0.5435 time: 0.0034s\n",
      "Epoch: 0159 loss_train: 0.6958 acc_train: 0.5300 loss_val: 0.6909 acc_val: 0.5435 time: 0.0038s\n",
      "Epoch: 0160 loss_train: 0.6900 acc_train: 0.5530 loss_val: 0.6909 acc_val: 0.5435 time: 0.0031s\n",
      "Epoch: 0161 loss_train: 0.6963 acc_train: 0.5440 loss_val: 0.6909 acc_val: 0.5435 time: 0.0032s\n",
      "Epoch: 0162 loss_train: 0.7103 acc_train: 0.5440 loss_val: 0.6909 acc_val: 0.5435 time: 0.0031s\n",
      "Epoch: 0163 loss_train: 0.6944 acc_train: 0.5450 loss_val: 0.6909 acc_val: 0.5430 time: 0.0032s\n",
      "Epoch: 0164 loss_train: 0.6910 acc_train: 0.5320 loss_val: 0.6909 acc_val: 0.5430 time: 0.0031s\n",
      "Epoch: 0165 loss_train: 0.7009 acc_train: 0.5350 loss_val: 0.6909 acc_val: 0.5430 time: 0.0032s\n",
      "Epoch: 0166 loss_train: 0.6976 acc_train: 0.5290 loss_val: 0.6909 acc_val: 0.5430 time: 0.0031s\n",
      "Epoch: 0167 loss_train: 0.7022 acc_train: 0.5270 loss_val: 0.6909 acc_val: 0.5430 time: 0.0033s\n",
      "Epoch: 0168 loss_train: 0.6992 acc_train: 0.5280 loss_val: 0.6909 acc_val: 0.5430 time: 0.0032s\n",
      "Epoch: 0169 loss_train: 0.6917 acc_train: 0.5520 loss_val: 0.6909 acc_val: 0.5430 time: 0.0030s\n",
      "Epoch: 0170 loss_train: 0.6920 acc_train: 0.5240 loss_val: 0.6909 acc_val: 0.5430 time: 0.0031s\n",
      "Epoch: 0171 loss_train: 0.6951 acc_train: 0.5330 loss_val: 0.6909 acc_val: 0.5435 time: 0.0030s\n",
      "Epoch: 0172 loss_train: 0.6871 acc_train: 0.5570 loss_val: 0.6909 acc_val: 0.5440 time: 0.0031s\n",
      "Epoch: 0173 loss_train: 0.6939 acc_train: 0.5410 loss_val: 0.6909 acc_val: 0.5425 time: 0.0032s\n",
      "Epoch: 0174 loss_train: 0.6881 acc_train: 0.5520 loss_val: 0.6909 acc_val: 0.5420 time: 0.0030s\n",
      "Epoch: 0175 loss_train: 0.6982 acc_train: 0.5180 loss_val: 0.6909 acc_val: 0.5415 time: 0.0029s\n",
      "Epoch: 0176 loss_train: 0.6954 acc_train: 0.5460 loss_val: 0.6909 acc_val: 0.5425 time: 0.0031s\n",
      "Epoch: 0177 loss_train: 0.6926 acc_train: 0.5370 loss_val: 0.6909 acc_val: 0.5425 time: 0.0031s\n",
      "Epoch: 0178 loss_train: 0.6967 acc_train: 0.5230 loss_val: 0.6909 acc_val: 0.5425 time: 0.0030s\n",
      "Epoch: 0179 loss_train: 0.6911 acc_train: 0.5400 loss_val: 0.6909 acc_val: 0.5425 time: 0.0031s\n",
      "Epoch: 0180 loss_train: 0.6935 acc_train: 0.5590 loss_val: 0.6909 acc_val: 0.5425 time: 0.0033s\n",
      "Epoch: 0181 loss_train: 0.6907 acc_train: 0.5360 loss_val: 0.6909 acc_val: 0.5425 time: 0.0027s\n",
      "Epoch: 0182 loss_train: 0.7003 acc_train: 0.5440 loss_val: 0.6909 acc_val: 0.5425 time: 0.0030s\n",
      "Epoch: 0183 loss_train: 0.6958 acc_train: 0.5360 loss_val: 0.6909 acc_val: 0.5430 time: 0.0028s\n",
      "Epoch: 0184 loss_train: 0.6913 acc_train: 0.5510 loss_val: 0.6910 acc_val: 0.5425 time: 0.0029s\n",
      "Epoch: 0185 loss_train: 0.6905 acc_train: 0.5310 loss_val: 0.6910 acc_val: 0.5420 time: 0.0031s\n",
      "Epoch: 0186 loss_train: 0.6900 acc_train: 0.5490 loss_val: 0.6910 acc_val: 0.5420 time: 0.0026s\n",
      "Epoch: 0187 loss_train: 0.6963 acc_train: 0.5260 loss_val: 0.6910 acc_val: 0.5425 time: 0.0027s\n",
      "Epoch: 0188 loss_train: 0.6935 acc_train: 0.5330 loss_val: 0.6910 acc_val: 0.5425 time: 0.0027s\n",
      "Epoch: 0189 loss_train: 0.6893 acc_train: 0.5550 loss_val: 0.6911 acc_val: 0.5420 time: 0.0026s\n",
      "Epoch: 0190 loss_train: 0.6957 acc_train: 0.5390 loss_val: 0.6911 acc_val: 0.5415 time: 0.0027s\n",
      "Epoch: 0191 loss_train: 0.6917 acc_train: 0.5470 loss_val: 0.6911 acc_val: 0.5415 time: 0.0025s\n",
      "Epoch: 0192 loss_train: 0.6946 acc_train: 0.5550 loss_val: 0.6911 acc_val: 0.5425 time: 0.0030s\n",
      "Epoch: 0193 loss_train: 0.6961 acc_train: 0.5450 loss_val: 0.6911 acc_val: 0.5425 time: 0.0027s\n",
      "Epoch: 0194 loss_train: 0.6907 acc_train: 0.5280 loss_val: 0.6911 acc_val: 0.5425 time: 0.0024s\n",
      "Epoch: 0195 loss_train: 0.6910 acc_train: 0.5450 loss_val: 0.6911 acc_val: 0.5435 time: 0.0026s\n",
      "Epoch: 0196 loss_train: 0.6907 acc_train: 0.5260 loss_val: 0.6912 acc_val: 0.5435 time: 0.0025s\n",
      "Epoch: 0197 loss_train: 0.6900 acc_train: 0.5650 loss_val: 0.6912 acc_val: 0.5440 time: 0.0028s\n",
      "Epoch: 0198 loss_train: 0.6927 acc_train: 0.5350 loss_val: 0.6912 acc_val: 0.5435 time: 0.0025s\n",
      "Epoch: 0199 loss_train: 0.6950 acc_train: 0.5330 loss_val: 0.6912 acc_val: 0.5425 time: 0.0026s\n",
      "Epoch: 0200 loss_train: 0.6993 acc_train: 0.5350 loss_val: 0.6912 acc_val: 0.5425 time: 0.0025s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.7600s\n",
      "Test set results: loss= 0.6916 accuracy= 0.5470\n"
     ]
    }
   ],
   "source": [
    "train_test(A, X, y, idx_train, idx_val, idx_test,\n",
    "    no_cuda = False,\n",
    "    seed = 40,\n",
    "    epochs = 200,\n",
    "    learning_rate = 0.0001,\n",
    "    weight_decay = 5e-4,\n",
    "    hidden_units = 256,\n",
    "    dropout = 0.5,\n",
    "    type = \"FCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
